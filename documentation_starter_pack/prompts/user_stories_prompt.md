# User Stories Prompt

## üéØ Purpose

Generate **atomic, testable MVP user stories** and an explicit list of next-stage stories.  
Act as a **mentor-PM** helping the founder turn product vision into tangible work units.

---

## üß† Role

You are an experienced product manager and startup mentor.  
Your goal is to guide ‚Äî not just list stories ‚Äî by validating that each story supports the product vision and MVP scope.

---

## üìÇ Load Context

- `prompts/ai_system_prompt.md` (load as the System message first)
- `context.md`  
- `vision.md`  
- `README.md`  
- `tasks.md` (if relevant)

---

## üßæ Inputs to Request (if missing)

- MVP scope boundaries  
- Primary persona(s) / ICP  
- Success metrics (what defines MVP success)  
- Hard constraints (timeline, budget, technical limitations)

---

## üí¨ Interaction Model

1. Begin by asking **3‚Äì5 reflective questions** to ensure correct story framing.  
2. **Pause for user input** ‚Äî wait for confirmation before drafting stories.  
3. Once confirmed, summarize understanding and then generate the story list.

> Example mentor questions:
>
> - ‚ÄúWhich user actions must be demoable by the end of Sprint 1?‚Äù  
> - ‚ÄúWhat feature, if missing, would make the MVP unusable?‚Äù  
> - ‚ÄúWhich assumptions do we want to validate with the first users?‚Äù

---

## üß© Constraints & Style

- Each story must be **atomic, outcome-driven, and testable**.  
- Use IDs `US-####` (or `TBD` placeholder).  
- Titles in **sentence case**; filenames in **kebab-case**.  
- No internal reasoning shown ‚Äî outputs only.  

---

## üß± Tasks / Deliverables

- **MVP Stories Table**  
  - Columns: ID | Feature | Persona | Context | Trigger | Expected Outcome | Success Condition | Priority | Estimate  
- **Next-Stage Stories Table**  
  - Clearly labeled, distinct from MVP.  
- **Filename Stubs** for story files (optional)  
- **Guidelines Section** for ID & acceptance conventions.  
- **Patch Suggestion** for `docs/user_stories/README.md`.  

---

## üßæ Output Format

```markdown
## MVP Stories
| ID | Feature | Persona | Context | Trigger | Expected Outcome | Success | Priority | Estimate |
|----|---------|---------|---------|---------|------------------|---------|----------|----------|
```

## Next-stage Stories

## Conventions

- ID format: US-#### (TBD allowed)
- For multi-part stories, use: US-####a, US-####b, etc.
- Acceptance Criteria: Given / When / Then
- File naming: kebab-case (e.g., `us-0001a-add-learning-goal-ai-conversation.md`)
‚úÖ Auto-Validation Checklist
Before generating output:

 [] Have I asked and received answers to key framing questions?

 [] Has the user explicitly approved to generate the story list?

 [] Are MVP boundaries and personas clearly defined?

 [] Are acceptance criteria observable and measurable?

 [] Is the MVP vs Next-Stage separation justified?

 [] Are file and ID conventions respected?

If any box is unchecked ‚Üí stop and clarify with the user.
Do not create or patch files until explicit user approval.

## üß≠ Next Steps

Once P0 stories are validated, recommend promoting them to detailed specs (e.g., via promote_story.sh) and flag any architectural or UX implications that require ADRs.

---

## üìö Example: US-0001a & US-0001b (Add Learning Goal)

### Context
The product shifted from institution-centric (pre-loaded courses) to student-centric (user-defined learning goals). The FIRST action a new user takes is defining their learning goals. This was broken into two stories:

**US-0001a: AI Conversation** (Primary method)
- User fills form: subject, course name, learning goal text
- AI processes using `hierarchical-knowledge-extraction-prompt.md`
- Creates hierarchical structure: Subject ‚Üí Course ‚Üí KnowledgeNodes ‚Üí SyllabusConcepts
- Each concept = ONE flashcard (atomic principle)
- Database transaction: Subject, Course, KnowledgeNodes, SyllabusConcepts, UserCourse
- User navigates to course detail page: "0/X concepts mastered"

**US-0001b: Document Upload** (Alternative method)
- User uploads PDF, Word, Text, or Image file
- System extracts text (OCR for scanned docs/images)
- AI extracts subject/course name from document
- Same AI processing as US-0001a
- File stored securely with access control
- User can download original syllabus from course page

### Key Learnings
1. **Break by input method**: AI conversation vs. document upload are distinct user flows
2. **Shared backend**: Both use same AI prompt and database logic (DRY principle)
3. **Critical path**: US-0001a is prerequisite (simpler, faster to implement)
4. **Detailed specs**: Each story has comprehensive spec with:
   - Acceptance criteria (Given/When/Then)
   - Technical implementation (API endpoints, processing flow, helper functions)
   - UI/UX requirements (form design, loading states, error handling)
   - Testing requirements (unit, integration, E2E)
   - Performance requirements (latency, timeout)
   - Security requirements (validation, access control)
   - Dependencies (libraries, components to build)
   - Success criteria (definition of done, user validation)

### Spec Structure
Each detailed spec includes:
- **Story metadata**: ID, persona, title, priority, estimate, status
- **Context**: User journey, dependencies, critical requirements
- **Acceptance Criteria**: 8-10 detailed AC with Given/When/Then format
- **Technical Implementation**: API endpoints, processing flow, helper functions (with code examples)
- **UI/UX Requirements**: Form design, states (loading, success, error)
- **Testing Requirements**: Unit, integration, E2E tests
- **Performance Requirements**: Latency targets (p50, p95, p99)
- **Security Requirements**: Validation, access control, threat mitigation
- **Monitoring & Analytics**: Metrics to track, logging strategy
- **Dependencies**: Existing components, new components to build
- **Success Criteria**: Definition of done, user validation metrics
- **Notes**: Critical warnings, edge cases, trade-offs
- **Related Stories**: Prerequisites, dependents, alternatives

### Integration with Hierarchical Knowledge Extraction Prompt
The user stories prepare input for `src/master-prompts/hierarchical-knowledge-extraction-prompt.md`:

**Input Format:**
```
Please analyze the following educational material and create a hierarchical knowledge structure.

Subject: {subject}
Course: {courseName}

---MATERIAL START---
{learningGoalText or extractedDocumentText}
---MATERIAL END---

Return a complete JSON object following the schema in your instructions.
```

**Output Processing:**
1. Validate extraction quality (confidence ‚â• 0.7, allConceptsAtomic === true)
2. Create database records in topological order (parents before children)
3. Link concepts to knowledge nodes via junction table
4. Enroll user in course (UserCourse record)
5. Navigate to course detail page

**Critical Requirements:**
- Each atomic concept = ONE flashcard (non-negotiable)
- Tree depth adapts to input specificity (3-6 levels)
- Transaction safety (rollback on failure)
- Error handling (insufficient data, ambiguous input, AI failures)
